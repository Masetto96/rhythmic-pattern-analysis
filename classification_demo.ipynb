{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Magnitude Transform for genre/rythmic pattern classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mirdata\n",
    "import librosa\n",
    "import helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groove Midi\n",
    "\n",
    "_Jon Gillick, Adam Roberts, Jesse Engel, Douglas Eck, and David Bamman.\n",
    "\"Learning to Groove with Inverse Sequence Transformations.\"\n",
    "International Conference on Machine Learning (ICML), 2019._\n",
    "\n",
    "The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.\n",
    "\n",
    "It could be used to classify fills or beats. Keep in mind that while fills tend to have a short duration (few seconds), beats tend to be longer. Therefore beats should be segmented in shorter chunks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groove_dataset = mirdata.initialize(\"groove_midi\")\n",
    "# groove_dataset.download()\n",
    "groove_dataset.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "track.style -> a string style for the performance formatted as “primary/secondary” (e.g. rock/halftime, funk/purdieshuffle). The primary style comes from the Genre List below.\n",
    "\n",
    "Genre List: afrobeat, afrocuban, blues, country, dance, funk, gospel, highlife, hiphop, jazz, latin, middleeastern, neworleans, pop, punk, reggae, rock, soul\n",
    "\n",
    "For the following experiment the label will consist of the primary style only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for _, track in tqdm(groove_dataset.load_tracks().items()):\n",
    "    if track.beat_type == \"fill\":\n",
    "        try:\n",
    "            y, sr = librosa.load(track.audio_path, sr=8000)\n",
    "            # stm = helpers.compute_stm(y=y, sr=sr, auto_cor_lag_seconds=2)[:30]\n",
    "            # features.append(stm)\n",
    "            features.append(helpers.compute_stm(y=y, sr=sr, win_size=256, hop=128, auto_cor_lag_seconds=5)[:90])\n",
    "            labels.append(track.style.split(\"/\")[0])\n",
    "        except Exception as e:\n",
    "            # exception encoutered with invalid audio_path\n",
    "            print(\"Error:\", e)\n",
    "            continue\n",
    "\n",
    "encoded_labels = LabelEncoder().fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(features), encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\")\n",
    "# rnn = RadiusNeighborsClassifier(n_neighbors=7, metric=\"cosine\").fit(X=X_train, y=y_train)\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(knn, features, encoded_labels, cv=k_fold, scoring=\"accuracy\")\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\").fit(X=x_train, y=y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(knn, x_test, y_test, display_labels=set(labels))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ballroom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [00:08<00:00, 80.14it/s] \n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'tracks': {}}, {'tracks': {}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ballroom_dataset = mirdata.initialize(\"ballroom\")\n",
    "# ballroom_dataset.download()\n",
    "ballroom_dataset.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for _, track in tqdm(ballroom_dataset.load_tracks().items()):\n",
    "    y, sr = librosa.load(track.audio_path, sr=8000)\n",
    "    # note that for this dataset, the number of coefs has been set to 170, as reported in the paper\n",
    "    features.append(helpers.compute_stm(y=y, sr=sr, win_size=256, hop=128, auto_cor_lag_seconds=5)[:90])\n",
    "    labels.append(track.genre)\n",
    "\n",
    "encoded_labels = LabelEncoder().fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(features), encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\")\n",
    "# rnc = RadiusNeighborsClassifier(metric=\"cosine\", radius=20)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "# k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(knn, features, labels, cv=loo, scoring=\"accuracy\")\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "knn = KNeighborsClassifier(metric=\"cosine\").fit(X=x_train, y=y_train)\n",
    "# rnc = RadiusNeighborsClassifier(metric=\"cosine\", radius=10).fit(X=x_train, y=y_train)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(knn, x_test, y_test, display_labels=set(labels))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch CV on Ballroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mels: 50 auto_cor_lag_seconds: 5 stm_coefs: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [01:30<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Accuracy: 0.7664756446991404\n",
      "n_mels: 50 auto_cor_lag_seconds: 5 stm_coefs: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [01:37<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Accuracy: 0.7578796561604585\n",
      "n_mels: 50 auto_cor_lag_seconds: 5 stm_coefs: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [01:28<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Accuracy: 0.7607449856733525\n",
      "n_mels: 50 auto_cor_lag_seconds: 5 stm_coefs: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 54/698 [00:07<01:23,  7.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, track \u001b[38;5;129;01min\u001b[39;00m tqdm(ballroom_dataset\u001b[38;5;241m.\u001b[39mload_tracks()\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     20\u001b[0m     y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(track\u001b[38;5;241m.\u001b[39maudio_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_stm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_cor_lag_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_cor_lag_seconds\u001b[49m\u001b[43m)\u001b[49m[:stm_coefs])\n\u001b[1;32m     22\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(track\u001b[38;5;241m.\u001b[39mgenre)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Encode labels\u001b[39;00m\n",
      "File \u001b[0;32m~/uni/internship/rythmic-pattern-analysis/helpers.py:12\u001b[0m, in \u001b[0;36mcompute_stm\u001b[0;34m(y, sr, win_size, hop, n_mels, auto_cor_lag_seconds)\u001b[0m\n\u001b[1;32m      8\u001b[0m     sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m\n\u001b[1;32m     10\u001b[0m y, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mtrim(y)\n\u001b[0;32m---> 12\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m oss \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39monset\u001b[38;5;241m.\u001b[39monset_strength(S\u001b[38;5;241m=\u001b[39mlibrosa\u001b[38;5;241m.\u001b[39mpower_to_db(mel, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax), sr\u001b[38;5;241m=\u001b[39msr, lag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, aggregate\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmedian)\n\u001b[1;32m     16\u001b[0m oss_ac \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mautocorrelate(oss, max_size\u001b[38;5;241m=\u001b[39m auto_cor_lag_seconds \u001b[38;5;241m*\u001b[39m sr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m hop)\n",
      "File \u001b[0;32m~/uni/internship/rythmic-pattern-analysis/.venv-rythm/lib/python3.11/site-packages/librosa/feature/spectral.py:2130\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelspectrogram\u001b[39m(\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2010\u001b[0m     y: Optional[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2020\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   2021\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute a mel-scaled spectrogram.\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m \n\u001b[1;32m   2024\u001b[0m \u001b[38;5;124;03m    If a spectrogram input ``S`` is provided, then it is mapped directly onto\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;124;03m    >>> ax.set(title='Mel-frequency spectrogram')\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2130\u001b[0m     S, n_fft \u001b[38;5;241m=\u001b[39m \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2142\u001b[0m     \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m   2143\u001b[0m     mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/uni/internship/rythmic-pattern-analysis/.venv-rythm/lib/python3.11/site-packages/librosa/core/spectrum.py:2822\u001b[0m, in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2817\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[1;32m   2818\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2819\u001b[0m         )\n\u001b[1;32m   2820\u001b[0m     S \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2821\u001b[0m         np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[0;32m-> 2822\u001b[0m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2831\u001b[0m         )\n\u001b[1;32m   2832\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m power\n\u001b[1;32m   2833\u001b[0m     )\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "File \u001b[0;32m~/uni/internship/rythmic-pattern-analysis/.venv-rythm/lib/python3.11/site-packages/librosa/core/spectrum.py:378\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[1;32m    376\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 378\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m \u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[0;32m~/uni/internship/rythmic-pattern-analysis/.venv-rythm/lib/python3.11/site-packages/numpy/fft/_pocketfft.py:409\u001b[0m, in \u001b[0;36mrfft\u001b[0;34m(a, n, axis, norm)\u001b[0m\n\u001b[1;32m    407\u001b[0m     n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m    408\u001b[0m inv_norm \u001b[38;5;241m=\u001b[39m _get_forward_norm(n, norm)\n\u001b[0;32m--> 409\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/uni/internship/rythmic-pattern-analysis/.venv-rythm/lib/python3.11/site-packages/numpy/fft/_pocketfft.py:73\u001b[0m, in \u001b[0;36m_raw_fft\u001b[0;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     a \u001b[38;5;241m=\u001b[39m swapaxes(a, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mpfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     r \u001b[38;5;241m=\u001b[39m swapaxes(r, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a grid of parameters to search over\n",
    "param_grid = {\n",
    "    'n_mels': [50, 75, 100],      # Example values for n_mels\n",
    "    'auto_cor_lag_seconds': [5, 8, 13],  # Example values for auto_cor_lag_seconds\n",
    "    'stm_coefs' : [i for i in range(30, 400, 30)]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for n_mels in param_grid['n_mels']:\n",
    "    for auto_cor_lag_seconds in param_grid['auto_cor_lag_seconds']:\n",
    "        for stm_coefs in param_grid['stm_coefs']:\n",
    "            print(\"n_mels:\", n_mels, \"auto_cor_lag_seconds:\", auto_cor_lag_seconds, \"stm_coefs:\", stm_coefs)\n",
    "            # Compute features with current parameters\n",
    "            features = []\n",
    "            labels = []\n",
    "            for _, track in tqdm(ballroom_dataset.load_tracks().items()):\n",
    "                y, sr = librosa.load(track.audio_path, sr=8000)\n",
    "                features.append(helpers.compute_stm(y=y, sr=sr, win_size=160, hop=80, n_mels=n_mels, auto_cor_lag_seconds=auto_cor_lag_seconds)[:stm_coefs])\n",
    "                labels.append(track.genre)\n",
    "\n",
    "            # Encode labels\n",
    "            encoded_labels = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "            # Initialize and evaluate the classifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\")\n",
    "            loo = LeaveOneOut()\n",
    "            cv_scores = cross_val_score(knn, features, labels, cv=loo, scoring=\"accuracy\")\n",
    "            mean_cv_score = cv_scores.mean()\n",
    "            print(\"Mean CV Accuracy:\", mean_cv_score)\n",
    "\n",
    "            # Update best score and parameters if needed\n",
    "            if mean_cv_score > best_score:\n",
    "                best_score = mean_cv_score\n",
    "                best_params = {\n",
    "                    'n_mels': n_mels,\n",
    "                    'auto_cor_lag_seconds': auto_cor_lag_seconds\n",
    "                }\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Mean CV Accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greek/Cretan Dances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cretan_dances_data_path = Path(\"../datasets/CretanDances\")\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for subfolder in cretan_dances_data_path.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        label = subfolder.name\n",
    "        print(label)\n",
    "        for audio_file in subfolder.glob(\"*.wav\"):\n",
    "            y, sr = librosa.load(audio_file, sr=None)\n",
    "            # note that for this dataset, the number of coefs has been set to 30, as reported in the paper\n",
    "            features.append(helpers.compute_stm(y=y, sr=sr, win_size=160, hop=160)[:30])\n",
    "            labels.append(label)\n",
    "\n",
    "encoded_labels = LabelEncoder().fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(features), encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\")\n",
    "# rnn = RadiusNeighborsClassifier(n_neighbors=7, metric=\"cosine\").fit(X=X_train, y=y_train)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "# k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(knn, features, labels, cv=loo, scoring=\"accuracy\")\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\").fit(X=x_train, y=y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(knn, x_test, y_test, display_labels=set(labels))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch CV on GreekDances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cretan_dances_data_path = Path(\"../datasets/CretanDances\")\n",
    "\n",
    "# Define a grid of parameters to search over\n",
    "param_grid = {\n",
    "    'n_mels': [50, 75, 100],      # Example values for n_mels\n",
    "    'auto_cor_lag_seconds': [5, 8, 13],  # Example values for auto_cor_lag_seconds\n",
    "    'stm_coefs' : [i for i in range(30, 400, 30)]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for n_mels in param_grid['n_mels']:\n",
    "    for auto_cor_lag_seconds in param_grid['auto_cor_lag_seconds']:\n",
    "        for stm_coefs in param_grid['stm_coefs']:\n",
    "            print(\"n_mels:\", n_mels, \"auto_cor_lag_seconds:\", auto_cor_lag_seconds, \"stm_coefs:\", stm_coefs)\n",
    "            # Compute features with current parameters\n",
    "            features = []\n",
    "            labels = []\n",
    "            for subfolder in cretan_dances_data_path.iterdir():\n",
    "                if subfolder.is_dir():\n",
    "                    label = subfolder.name\n",
    "                    print(label)\n",
    "                    for audio_file in subfolder.glob(\"*.wav\"):\n",
    "                        y, sr = librosa.load(audio_file, sr=8000)\n",
    "                        features.append(helpers.compute_stm(y=y, sr=sr, win_size=160, hop=80, n_mels=n_mels, auto_cor_lag_seconds=auto_cor_lag_seconds)[:stm_coefs])\n",
    "                        labels.append(label)\n",
    "\n",
    "\n",
    "            # Encode labels\n",
    "            encoded_labels = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "            # Initialize and evaluate the classifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=7, metric=\"cosine\")\n",
    "            loo = LeaveOneOut()\n",
    "            cv_scores = cross_val_score(knn, features, labels, cv=loo, scoring=\"accuracy\")\n",
    "            mean_cv_score = cv_scores.mean()\n",
    "            print(\"Mean CV Accuracy:\", mean_cv_score)\n",
    "\n",
    "            # Update best score and parameters if needed\n",
    "            if mean_cv_score > best_score:\n",
    "                best_score = mean_cv_score\n",
    "                best_params = {\n",
    "                    'n_mels': n_mels,\n",
    "                    'auto_cor_lag_seconds': auto_cor_lag_seconds\n",
    "                }\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Mean CV Accuracy:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-rythm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
